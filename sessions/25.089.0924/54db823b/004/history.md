
## train_1

**input:**
```
0 3 3 9 3 3 0 0 0 0 0 0 0 0 0
0 3 3 3 3 3 0 3 9 3 3 0 0 0 0
0 9 3 3 9 3 0 3 3 3 9 0 0 0 0
0 0 0 0 0 0 0 9 3 9 3 0 0 0 0
0 0 0 0 0 0 0 3 3 3 3 0 0 0 0
3 3 9 0 0 0 0 0 0 0 0 0 0 0 0
9 3 3 0 0 3 9 3 0 0 0 3 3 9 3
3 9 3 0 0 3 3 9 0 0 0 9 3 3 3
3 3 3 0 0 0 0 0 0 0 0 3 3 9 3
9 3 3 0 0 0 0 0 0 0 0 3 9 3 3
3 3 9 0 3 3 3 9 3 0 0 3 3 9 3
0 0 0 0 9 3 9 3 3 0 0 3 3 3 9
0 0 0 0 3 3 3 3 3 0 0 9 3 3 3
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


**output:**
```
0 3 3 9 3 3 0 0 0 0 0 0 0 0 0
0 3 3 3 3 3 0 3 9 3 3 0 0 0 0
0 9 3 9 3 3 0 3 3 3 9 0 0 0 0
0 0 0 0 0 0 0 9 3 9 3 0 0 0 0
0 0 0 0 0 0 0 3 3 3 3 0 0 0 0
3 3 9 0 0 0 0 0 0 0 0 0 0 0 0
9 3 3 0 0 0 0 0 0 0 0 3 3 9 3
3 9 3 0 0 0 0 0 0 0 0 9 3 3 3
3 3 3 0 0 0 0 0 0 0 0 3 3 9 3
9 3 3 0 0 0 0 0 0 0 0 3 9 3 3
3 3 9 0 3 3 3 9 3 0 0 3 3 9 3
0 0 0 0 9 3 9 3 3 0 0 3 3 3 9
0 0 0 0 3 3 3 3 3 0 0 9 3 3 3
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


## train_2

**input:**
```
0 3 3 3 0 0 0 0 0 0 0 0 0 0 0
0 9 3 9 0 0 3 3 3 3 3 0 0 0 0
0 3 9 3 0 0 3 3 3 9 3 0 0 0 0
0 3 3 9 0 0 3 3 3 3 3 0 0 0 0
0 3 3 3 0 0 3 3 3 3 3 0 0 0 0
0 0 0 0 0 0 3 3 3 3 3 0 0 0 0
0 0 3 9 3 0 0 0 0 0 0 0 0 0 0
0 0 9 3 3 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 3 3 3 3 3 3 3
0 3 3 3 3 3 0 0 3 3 3 3 3 3 3
0 3 3 3 3 3 0 0 3 9 3 9 9 3 3
0 3 9 3 3 3 0 0 3 3 3 3 3 3 3
0 3 3 3 3 3 0 0 3 3 9 3 3 3 3
0 3 3 3 9 3 0 0 3 3 3 3 3 3 9
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


**output:**
```
0 3 3 3 0 0 0 0 0 0 0 0 0 0 0
0 9 3 9 0 0 0 0 0 0 0 0 0 0 0
0 3 9 3 0 0 0 0 0 0 0 0 0 0 0
0 3 3 9 0 0 0 0 0 0 0 0 0 0 0
0 3 3 3 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 3 9 3 0 0 0 0 0 0 0 0 0 0
0 0 9 3 3 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 3 3 3 3 3 3 3
0 3 3 3 3 3 0 0 3 3 3 3 3 3 3
0 3 3 3 3 3 0 0 3 9 3 9 9 3 3
0 3 9 3 3 3 0 0 3 3 3 3 3 3 3
0 3 3 3 3 3 0 0 3 3 9 3 3 3 3
0 3 3 3 9 3 0 0 3 3 3 3 3 3 9
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


## train_3

**input:**
```
0 0 0 0 0 0 0 0 0 0 0 0 3 3 3
0 3 3 3 3 3 0 0 0 0 0 0 3 9 3
0 3 3 9 3 3 0 3 3 3 3 0 3 9 3
0 3 9 3 3 3 0 3 9 9 3 0 3 3 9
0 0 0 0 0 0 0 3 3 3 3 0 9 3 3
0 0 0 0 0 0 0 3 3 9 3 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
3 9 3 3 3 0 0 0 0 0 0 0 3 3 0
3 3 3 9 3 0 0 0 0 0 0 0 3 9 0
3 9 3 3 3 0 0 3 3 3 3 0 3 3 0
3 3 3 9 3 0 0 3 9 3 9 0 3 9 0
0 0 0 0 0 0 0 3 3 3 3 0 9 3 0
0 0 0 0 0 0 0 3 9 9 3 0 0 0 0
0 0 0 0 0 0 0 3 3 3 9 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


**output:**
```
0 0 0 0 0 0 0 0 0 0 0 0 3 3 3
0 0 0 0 0 0 0 0 0 0 0 0 3 9 3
0 0 0 0 0 0 0 3 3 3 3 0 3 9 3
0 0 0 0 0 0 0 3 9 9 3 0 3 3 9
0 0 0 0 0 0 0 3 3 3 3 0 9 3 3
0 0 0 0 0 0 0 3 3 9 3 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
3 9 3 3 3 0 0 0 0 0 0 0 3 3 0
3 3 3 9 3 0 0 0 0 0 0 0 3 9 0
3 9 3 3 3 0 0 3 3 3 3 0 3 3 0
3 3 3 9 3 0 0 3 9 3 9 0 3 9 0
0 0 0 0 0 0 0 3 3 3 3 0 9 3 0
0 0 0 0 0 0 0 3 9 9 3 0 0 0 0
0 0 0 0 0 0 0 3 3 3 9 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


## train_4

**input:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
3 3 9 3 3 3 3 0 0 0 3 3 9 3 9
3 3 3 3 3 9 3 0 0 0 3 3 3 3 3
3 9 3 9 3 3 3 0 0 0 3 9 3 3 9
3 3 3 3 9 3 3 0 0 0 3 3 3 3 3
3 3 9 3 3 3 3 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 3 3 3 3 3 3 3 0 0
0 3 3 3 3 0 3 3 3 3 3 9 3 0 0
0 3 9 3 9 0 3 9 3 3 3 3 3 0 0
0 3 3 3 9 0 3 3 3 9 3 3 3 0 0
0 9 3 9 3 0 3 3 3 3 3 3 3 0 0
0 3 3 3 3 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


**output:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
3 3 9 3 3 3 3 0 0 0 3 3 9 3 9
3 3 3 3 3 9 3 0 0 0 3 3 3 3 3
3 9 3 9 3 3 3 0 0 0 3 9 3 3 9
3 3 3 3 9 3 3 0 0 0 3 3 3 3 3
3 3 9 3 3 3 3 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 3 3 3 3 0 0 0 0 0 0 0 0 0 0
0 3 9 3 9 0 0 0 0 0 0 0 0 0 0
0 3 3 3 9 0 0 0 0 0 0 0 0 0 0
0 9 3 9 3 0 0 0 0 0 0 0 0 0 0
0 3 3 3 3 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```

Okay, the `skimage` library is not available. I need to implement the object finding logic myself or use a simpler approach.

Let's rethink the object identification. We need contiguous blocks (connectivity=1 means touching sides, not diagonals) of non-white pixels. A simple flood fill or BFS/DFS approach can work.

**Revised Plan:**
1. Iterate through the grid.
2. If a non-white pixel is found that hasn't been visited yet, start a search (BFS or DFS) from this pixel to find all connected non-white pixels (using 4-way adjacency).
3. Mark visited pixels during the search.
4. Store the coordinates, total count, and maroon count for each object found.
5. After finding all objects, apply Hypothesis 5:
    a. Find the minimum maroon count.
    b. Filter objects with this minimum count.
    c. If > 1, filter further by maximum total size.
    d. If still > 1 (tie in size), filter by top-most, then left-most coordinate of the object's pixels. (The `tool_code` implicitly did this via sorting, let's make it explicit).
    e. Identify the coordinates of the selected object.
    f. Create the output grid by copying the input and setting the selected object's pixels to white (0).

Let's manually apply this to Train 4 again, as the tool failed.

**Train 4 Manual Check (Hypothesis 5):**
- Input:

```
. . . . . . . . . . . . . . .
G G M G G G G . . . G G M G M  <- Blob 1 (TL: 1,0), Blob 2 (TL: 1,10)
G G G G G M G . . . G G G G G
G M G M G G G . . . G M G G M
G G G G M G G . . . G G G G G
G G M G G G G . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . G G G G G G G . .  <- Blob 4 (TL: 7,6)
. G G G G . G G G G G M G . .  <- Blob 3 (TL: 8,1)
. G M G M . G M G G G G G . .
. G G G M . G G G M G G G . .
. M G M G . G G G G G G G . .
. G G G G . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
```

(G=Green(3), M=Maroon(9), .=White(0))

- **Object Identification & Counts (using BFS/DFS mentally):**
    - **Blob 1** (Starts 1,0): Extends to (5,6). Maroon count = 6 (1,2; 2,5; 3,1; 3,3; 4,4; 5,2). Total size = 29.
    - **Blob 2** (Starts 1,10): Extends to (4,14). Maroon count = 3 (1,12; 1,14; 3,12). Total size = 15.
    - **Blob 3** (Starts 8,1): Extends to (12,4). Maroon count = 4 (9,2; 9,4; 10,4; 11,1; 11,3). Total size = 17.
    - **Blob 4** (Starts 7,6): Extends to (11,12). Maroon count = 3 (8,11; 9,7; 10,9). Total size = 28.

- **Apply Hypothesis 5:**
    - Maroon counts: {6, 3, 4, 3}
    - Minimum maroon count = 3.
    - Objects with min maroon count: Blob 2, Blob 4.
    - Sizes of these objects: Blob 2 = 15, Blob 4 = 28.
    - Maximum size among these = 28.
    - Object with max size: Blob 4.
    - Select Blob 4.
    - Remove Blob 4.

- **Compare with Train 4 Output:**
    - The output grid shows the object starting at (7,6) (Blob 4) removed.
- **Conclusion:** Hypothesis 5 correctly predicts the transformation for all 4 training examples.

Now, formulate the perception, facts, and natural language program based on Hypothesis 5.

**Perception:**
- The input grid contains a white background (color 0) and several distinct objects composed of green (color 3) and maroon (color 9) pixels.
- Objects are defined as contiguous blocks of non-white pixels (using orthogonal adjacency).
- The transformation involves identifying and removing exactly one of these objects.
- The criteria for selecting the object to remove depends first on the count of maroon pixels within the object. The object(s) with the minimum number of maroon pixels are considered.
- If there's a tie (multiple objects have the same minimum number of maroon pixels), the tie is broken by selecting the object with the largest total number of pixels (area).
- If there's still a tie (same minimum maroon count and same maximum area), the tie is broken by selecting the object whose top-most pixel has the smallest row index, and then the smallest column index (top-left rule).
- The selected object is entirely removed, meaning all its pixels (both green and maroon) are changed to white (color 0). The rest of the grid remains unchanged.

**Facts (YAML):**

```yaml
elements:
  - object: grid
    pixels:
      - color: white (0) - background
      - color: green (3)
      - color: maroon (9)
  - object: blobs
    definition: Contiguous groups of non-white pixels (green or maroon), connected orthogonally.
    properties:
      - maroon_count: number of maroon (9) pixels in the blob.
      - total_size: total number of pixels (green + maroon) in the blob.
      - location: defined by the set of coordinates occupied by the blob pixels.
      - top_left_coordinate: the coordinate (row, col) of the pixel in the blob with the minimum row index, and among those, the minimum column index.

action:
  - name: identify_target_blob
    criteria:
      1. Find the minimum `maroon_count` (min_mc) among all blobs.
      2. Filter blobs where `maroon_count == min_mc`.
      3. If multiple blobs remain, find the maximum `total_size` (max_ts) among the filtered blobs.
      4. Filter blobs where `total_size == max_ts`.
      5. If multiple blobs remain, select the one with the minimum `top_left_coordinate` (first by row, then by column).
  - name: remove_blob
    target: the blob identified by `identify_target_blob`.
    effect: Change the color of all pixels belonging to the target blob to white (0).

relationship:
  - The output grid is identical to the input grid, except for the pixels belonging to the removed blob, which are turned white.
```


**Natural Language Program:**
1.  Identify all separate objects in the input grid. An object is a group of connected green (3) or maroon (9) pixels where connection is defined by sharing an edge (up, down, left, or right). Ignore white (0) background pixels.
2.  For each identified object, calculate two properties:
    *   `maroon_count`: The total number of maroon (9) pixels within the object.
    *   `total_size`: The total number of pixels (green and maroon) within the object.
3.  Determine the minimum `maroon_count` found among all objects.
4.  Select all objects that have this minimum `maroon_count`.
5.  If only one object was selected in step 4, this is the target object. Proceed to step 8.
6.  If multiple objects were selected in step 4, compare their `total_size`. Find the maximum `total_size` among these selected objects.
7.  Select the object(s) from step 6 that have this maximum `total_size`. If there is only one, this is the target object. If there is still a tie (multiple objects with the same minimum `maroon_count` and same maximum `total_size`), select the object whose top-most, left-most pixel comes first in reading order (minimum row index, then minimum column index). This is the target object.
8.  Create the output grid by copying the input grid.
9.  Change the color of every pixel belonging to the target object to white (0) in the output grid.**Perception**

1.  **Grid Composition**: The input and output grids are composed of pixels with three distinct colors: white (0), green (3), and maroon (9). White acts as the background.
2.  **Object Identification**: The non-white pixels (green and maroon) form distinct contiguous objects or "blobs". Contiguity is defined by orthogonal adjacency (sharing a side, not just a corner).
3.  **Transformation**: The core transformation involves identifying and removing exactly one of these non-white objects from the input grid to produce the output grid. Removal means changing all pixels constituting the object to the background color (white, 0).
4.  **Selection Criteria**: The specific object chosen for removal is determined by a hierarchical set of criteria applied to all objects found in the input:
    *   **Primary Criterion**: The number of maroon (9) pixels within the object. Objects with the *minimum* count of maroon pixels are prioritized.
    *   **Secondary Criterion (Tie-breaker 1)**: If multiple objects share the same minimum maroon pixel count, the object with the *maximum* total size (total count of green and maroon pixels) is chosen.
    *   **Tertiary Criterion (Tie-breaker 2)**: If a tie persists after considering both maroon count and total size, the object whose top-most, left-most pixel appears first in standard reading order (minimum row index, then minimum column index) is selected.
5.  **Output Generation**: The output grid is a direct copy of the input grid, except that all pixels belonging to the single selected object are replaced with white (0).

**Facts**


```yaml
elements:
  - object: grid
    attributes:
      - width: Integer
      - height: Integer
      - pixels: 2D array of integers (0-9)
    colors_present: [white (0), green (3), maroon (9)]
  - object: shapes
    description: Contiguous areas of non-white pixels within the grid.
    connectivity: Orthogonal (4-way adjacency)
    composition: Made of green (3) and/or maroon (9) pixels.
    properties:
      - maroon_count: The number of maroon (9) pixels in the shape.
      - total_size: The total number of pixels (green and maroon) in the shape.
      - location: The set of coordinates (row, column) occupied by the shape.
      - top_left: The coordinate (min_row, min_col) representing the top-most, left-most pixel of the shape.

actions:
  - name: find_all_shapes
    input: grid
    output: list of shapes with their properties (maroon_count, total_size, location, top_left)
  - name: select_shape_for_removal
    input: list of shapes
    output: the single shape selected for removal
    logic:
      1. Calculate `min_maroon_count` = minimum `maroon_count` across all shapes.
      2. Filter shapes to keep only those where `shape.maroon_count == min_maroon_count`. Let this be `candidates_1`.
      3. If `len(candidates_1) == 1`, return the single shape.
      4. Else, calculate `max_total_size` = maximum `total_size` among shapes in `candidates_1`.
      5. Filter `candidates_1` to keep only those where `shape.total_size == max_total_size`. Let this be `candidates_2`.
      6. If `len(candidates_2) == 1`, return the single shape.
      7. Else, sort `candidates_2` based on `top_left` coordinates (first by row, then by column). Return the first shape in the sorted list.
  - name: generate_output_grid
    input:
      - input_grid: original grid
      - shape_to_remove: the shape selected by `select_shape_for_removal`
    output: output_grid
    logic:
      1. Create a copy of the `input_grid`.
      2. For each pixel coordinate in `shape_to_remove.location`:
         Set the pixel at that coordinate in the copied grid to white (0).
      3. Return the modified grid copy.

relationships:
  - The transformation identifies shapes based on connectivity and color composition.
  - The selection process prioritizes minimizing maroon pixels, then maximizing total size, then top-left position.
  - The final output differs from the input only by the removal (conversion to white) of the single selected shape.
```


**Natural Language Program**

1.  **Identify Objects**: Scan the input grid to find all distinct connected objects composed of non-background colors (green '3' and maroon '9'). Use orthogonal adjacency (up, down, left, right) to determine connectivity.
2.  **Calculate Properties**: For each object found:
    *   Count the number of maroon ('9') pixels it contains (`maroon_count`).
    *   Count the total number of pixels it contains (`total_size`).
    *   Determine its top-left bounding pixel (the pixel with the minimum row index, breaking ties with the minimum column index).
3.  **Filter by Minimum Maroon Count**: Find the minimum `maroon_count` among all identified objects. Create a list containing only the objects that have this minimum count.
4.  **Filter by Maximum Size (if necessary)**: If the list from step 3 contains more than one object, find the maximum `total_size` among the objects in this list. Update the list to contain only the objects that have this maximum size.
5.  **Filter by Position (if necessary)**: If the list from step 4 still contains more than one object, select the single object whose top-left bounding pixel has the smallest row index. If there's still a tie (same minimum row index), select the one among the tied objects whose top-left bounding pixel has the smallest column index.
6.  **Generate Output**: Create a copy of the input grid. For the single object selected in the previous steps, change the color of all pixels belonging to that object to white ('0') in the copied grid. The resulting grid is the output.
```python
import numpy as np
from collections import deque

"""
The transformation identifies all distinct connected objects composed of non-background colors (green '3' and maroon '9') using orthogonal adjacency. For each object, it calculates the count of maroon pixels (`maroon_count`) and the total pixel count (`total_size`). It then selects a single object for removal based on a hierarchical criteria: 
1. Minimum `maroon_count`.
2. If tied, maximum `total_size`.
3. If still tied, the object whose top-most, left-most pixel appears first (minimum row index, then minimum column index).
Finally, it creates the output grid by copying the input grid and changing all pixels of the selected object to the background color (white '0').
"""

def find_objects(grid):
    """
    Identifies all distinct connected objects of non-background pixels in the grid.

    Args:
        grid: A numpy array representing the input grid.

    Returns:
        A list of dictionaries, where each dictionary represents an object
        and contains:
        'pixels': A set of (row, col) tuples for the object.
        'maroon_count': Integer count of maroon (9) pixels.
        'total_size': Integer count of all pixels.
        'top_left': Tuple (row, col) of the top-most, left-most pixel.
    """
    rows, cols = grid.shape
    visited = np.zeros_like(grid, dtype=bool)
    objects = []
    
    for r in range(rows):
        for c in range(cols):
            # If pixel is non-background and not visited, start BFS
            if grid[r, c] != 0 and not visited[r, c]:
                current_object_pixels = set()
                maroon_count = 0
                q = deque([(r, c)])
                visited[r, c] = True
                min_r, min_c = r, c # Track top-left for this object

                while q:
                    curr_r, curr_c = q.popleft()
                    current_object_pixels.add((curr_r, curr_c))

                    # Update top-left tracking
                    if curr_r < min_r:
                        min_r, min_c = curr_r, curr_c
                    elif curr_r == min_r and curr_c < min_c:
                        min_c = curr_c

                    # Count maroon pixels
                    if grid[curr_r, curr_c] == 9:
                        maroon_count += 1

                    # Explore neighbors (orthogonal)
                    for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:
                        nr, nc = curr_r + dr, curr_c + dc
                        # Check bounds, non-background color, and not visited
                        if 0 <= nr < rows and 0 <= nc < cols and \
                           grid[nr, nc] != 0 and not visited[nr, nc]:
                            visited[nr, nc] = True
                            q.append((nr, nc))
                
                # Store object details
                objects.append({
                    'pixels': current_object_pixels,
                    'maroon_count': maroon_count,
                    'total_size': len(current_object_pixels),
                    'top_left': (min_r, min_c)
                })
                
    return objects

def transform(input_grid):
    """
    Applies the transformation rule to the input grid.
    """
    # Convert input list of lists to numpy array for easier processing
    grid = np.array(input_grid, dtype=int)
    
    # 1. Identify all objects and their properties
    objects = find_objects(grid)
    
    # If no objects found, return the original grid
    if not objects:
        return input_grid
        
    # 2. Filter by Minimum Maroon Count
    min_maroon_count = min(obj['maroon_count'] for obj in objects)
    candidates = [obj for obj in objects if obj['maroon_count'] == min_maroon_count]
    
    target_object = None
    # 3. If only one candidate, it's the target
    if len(candidates) == 1:
        target_object = candidates[0]
    else:
        # 4. Filter by Maximum Size (if necessary)
        max_total_size = max(obj['total_size'] for obj in candidates)
        candidates = [obj for obj in candidates if obj['total_size'] == max_total_size]
        
        # 5. If only one candidate, it's the target
        if len(candidates) == 1:
            target_object = candidates[0]
        else:
            # 6. Filter by Position (if necessary)
            # Sort by top_left (row first, then column)
            candidates.sort(key=lambda obj: (obj['top_left'][0], obj['top_left'][1]))
            target_object = candidates[0] # The first one after sorting is the target

    # 7. Generate Output grid
    output_grid = grid.copy()
    
    # Change pixels of the target object to white (0)
    for r, c in target_object['pixels']:
        output_grid[r, c] = 0
        
    # Convert output numpy array back to list of lists
    return output_grid.tolist()

```
```python
import numpy as np
from collections import deque

"""
The transformation identifies all distinct connected objects composed of non-background colors (green '3' and maroon '9') using orthogonal adjacency. For each object, it calculates the count of maroon pixels (`maroon_count`) and the total pixel count (`total_size`), and identifies its top-left pixel coordinate. It then selects a single object for removal based on a hierarchical criteria: 
1. Minimum `maroon_count`.
2. If tied, maximum `total_size`.
3. If still tied, the object whose top-most, left-most pixel appears first (minimum row index, then minimum column index).
Finally, it creates the output grid by copying the input grid and changing all pixels of the selected object to the background color (white '0').
"""

def find_objects(grid):
    """
    Identifies all distinct connected objects of non-background pixels in the grid.

    Args:
        grid: A numpy array representing the input grid.

    Returns:
        A list of dictionaries, where each dictionary represents an object
        and contains:
        'pixels': A set of (row, col) tuples for the object.
        'maroon_count': Integer count of maroon (9) pixels.
        'total_size': Integer count of all pixels.
        'top_left': Tuple (row, col) of the top-most, left-most pixel.
    """
    rows, cols = grid.shape
    visited = np.zeros_like(grid, dtype=bool)
    objects = []
    
    for r in range(rows):
        for c in range(cols):
            # If pixel is non-background (not 0) and not visited, start BFS
            if grid[r, c] != 0 and not visited[r, c]:
                current_object_pixels = set()
                maroon_count = 0
                q = deque([(r, c)])
                visited[r, c] = True
                min_r, min_c = r, c # Track top-left for this object

                while q:
                    curr_r, curr_c = q.popleft()
                    current_object_pixels.add((curr_r, curr_c))

                    # Update top-left tracking: find min row, then min col for that row
                    if curr_r < min_r:
                        min_r, min_c = curr_r, curr_c
                    elif curr_r == min_r and curr_c < min_c:
                        min_c = curr_c

                    # Count maroon pixels
                    if grid[curr_r, curr_c] == 9:
                        maroon_count += 1

                    # Explore neighbors (orthogonal: up, down, left, right)
                    for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:
                        nr, nc = curr_r + dr, curr_c + dc
                        # Check bounds, non-background color, and not visited
                        if 0 <= nr < rows and 0 <= nc < cols and \
                           grid[nr, nc] != 0 and not visited[nr, nc]:
                            visited[nr, nc] = True
                            q.append((nr, nc))
                
                # Store object details after BFS completes for this object
                objects.append({
                    'pixels': current_object_pixels,
                    'maroon_count': maroon_count,
                    'total_size': len(current_object_pixels),
                    'top_left': (min_r, min_c)
                })
                
    return objects

def transform(input_grid):
    """
    Applies the transformation rule to the input grid.
    """
    # Convert input list of lists to numpy array for easier processing
    grid = np.array(input_grid, dtype=int)
    
    # 1. Identify all objects and calculate their properties
    objects = find_objects(grid)
    
    # Handle edge case: If no non-background objects found, return the original grid
    if not objects:
        # Ensure output is list of lists
        return grid.tolist() 
        
    # 2. Select Target Object: Filter by Minimum Maroon Count
    min_maroon_count = min(obj['maroon_count'] for obj in objects)
    candidates = [obj for obj in objects if obj['maroon_count'] == min_maroon_count]
    
    target_object = None
    # 3. If only one candidate after step 2, it's the target
    if len(candidates) == 1:
        target_object = candidates[0]
    else:
        # 4. Tie-breaker 1: Filter by Maximum Size (if necessary)
        max_total_size = max(obj['total_size'] for obj in candidates)
        candidates = [obj for obj in candidates if obj['total_size'] == max_total_size]
        
        # 5. If only one candidate after step 4, it's the target
        if len(candidates) == 1:
            target_object = candidates[0]
        else:
            # 6. Tie-breaker 2: Filter by Position (if necessary)
            # Sort remaining candidates by top_left coordinate (row first, then column)
            candidates.sort(key=lambda obj: (obj['top_left'][0], obj['top_left'][1]))
            # The first object after sorting is the target
            target_object = candidates[0] 

    # 7. Generate Output grid: Create a copy of the input grid
    output_grid = grid.copy()
    
    # 8. Modify Output grid: Change pixels of the target object to white (0)
    for r, c in target_object['pixels']:
        output_grid[r, c] = 0
        
    # Convert output numpy array back to list of lists for the required format
    return output_grid.tolist()
```
