
## train_1

**input:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


**output:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 8 8 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 8 8 8 8 0 0 0 0 0 0 0 0
0 0 0 0 0 0 8 8 8 8 8 8 0 0 0 0 0 0 0
0 0 0 0 0 8 8 8 8 8 8 8 8 0 0 0 0 0 0
0 0 0 0 8 8 8 8 8 8 8 8 8 8 0 0 0 0 0
0 0 0 8 8 8 8 8 8 8 8 8 8 8 8 0 0 0 0
0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0
0 0 0 8 8 8 8 8 8 8 8 8 8 8 8 0 0 0 0
0 0 0 0 8 8 8 8 8 8 8 8 8 8 0 0 0 0 0
0 0 0 0 0 8 8 8 8 8 8 8 8 0 0 0 0 0 0
0 0 0 0 0 0 8 8 8 8 8 8 0 0 0 0 0 0 0
0 0 0 0 0 0 0 8 8 8 8 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 8 8 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


## train_2

**input:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


**output:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 8 2 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 8 8 2 8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 8 2 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 8 8 8 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 8 8 8 8 8 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 8 8 8 8 8 8 8 0 0 0 0
0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 0 0 0
0 0 0 0 0 0 0 0 0 0 8 8 8 8 8 8 8 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 8 8 8 8 8 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 8 8 8 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```

Okay, let's break down the task based on the provided examples.

## Perception

1.  **Input Structure:** The input grids consist primarily of a white background (color 0) with one or more distinct objects composed of red pixels (color 2). These red objects appear as lines (horizontal or vertical) or potentially other contiguous shapes.
2.  **Output Structure:** The output grids retain the original red objects from the input. Additionally, new shapes composed of azure pixels (color 8) appear. These azure shapes surround the red objects.
3.  **Transformation:** The core transformation involves generating these azure shapes based on the properties of the red objects.
4.  **Shape Relationship:** The azure shapes resemble diamonds (or squares rotated 45 degrees) centered around the red objects. The size and extent of the azure diamond seem directly related to the corresponding red object.
5.  **Distance Metric:** The azure pixels appear to occupy locations that are within a certain *Manhattan distance* (sum of absolute differences in row and column coordinates) from the nearest red pixel of a given object.
6.  **Variable Threshold:** Comparing train_1 and train_2, the maximum distance for coloring a pixel azure changes. In train_1, the red line has 14 pixels, and the azure shape extends up to a Manhattan distance of 6. In train_2, the vertical red line (5 pixels) has an azure shape extending up to distance 2, and the horizontal red line (9 pixels) has an azure shape extending up to distance 4.
7.  **Rule Discovery:** This suggests a rule where the maximum allowed Manhattan distance (`k`) for painting azure pixels around a red object depends on the size (`L`, number of pixels) of that red object. The observed relationship is `k = floor((L - 1) / 2)`.
8.  **Process:** For each distinct red object, calculate its size `L`, determine the threshold `k`, and then color all white pixels azure if their minimum Manhattan distance to *any* pixel within *that specific* red object is greater than 0 and less than or equal to `k`.

## YAML Facts


```yaml
Initial_State:
  Grid:
    Contains:
      - Background:
          Color: white (0)
          Coverage: Most of the grid
      - Objects:
          Type: Connected Components
          Color: red (2)
          Quantity: One or more
          Shape: Primarily lines (horizontal/vertical), but can be any contiguous shape.
Final_State:
  Grid:
    Contains:
      - Background:
          Color: white (0)
      - Original_Objects:
          Type: Connected Components
          Color: red (2)
          Persistence: Unchanged from input
      - Added_Objects:
          Type: Area surrounding Original_Objects
          Color: azure (8)
          Shape: Diamond-like (related to Manhattan distance)
          Relationship_to_Original:
            - Each azure area corresponds to one red object.
            - Azure pixels occupy locations within a specific maximum Manhattan distance from the corresponding red object.
Transformation_Rule:
  Action: Add azure pixels based on red objects.
  Steps:
    - Identify distinct red (2) connected components (objects).
    - For each red object:
      - Calculate its size 'L' (pixel count).
      - Calculate a distance threshold 'k' using the formula: k = floor((L - 1) / 2).
      - Identify all pixels currently colored white (0).
      - For each white pixel:
        - Calculate the minimum Manhattan distance 'd_min' to any pixel within the current red object.
        - If 0 < d_min <= k:
          - Change the white pixel's color to azure (8).
  Properties_Used:
    - Pixel Color
    - Pixel Coordinates
    - Object Connectivity (Connected Components)
    - Object Size (Pixel Count)
    - Manhattan Distance
  Output_Color: azure (8) for added pixels.
  Input_Colors_Unaffected: red (2)
```


## Natural Language Program

1.  Create a copy of the input grid to serve as the output grid.
2.  Find all distinct connected groups (objects) of red (2) pixels in the input grid.
3.  For each identified red object:
    a. Count the number of pixels in this object. Let this count be `L`.
    b. Calculate a distance threshold `k` by taking the floor of `(L - 1) / 2`.
    c. Find the set of coordinates `(r, c)` for all pixels belonging to this red object.
    d. Iterate through every pixel location `(row, col)` in the grid.
    e. Check if the pixel at `(row, col)` in the *current output grid* is white (0).
    f. If it is white, calculate the Manhattan distance `d = |row - r| + |col - c|` for every red pixel coordinate `(r, c)` found in step 3c. Find the minimum `d_min` among these calculated distances.
    g. If `d_min` is greater than 0 AND `d_min` is less than or equal to the threshold `k` calculated in step 3b, change the color of the pixel at `(row, col)` in the output grid to azure (8).
4.  After processing all red objects, return the modified output grid.
```python
import numpy as np
from scipy.ndimage import label
import math

"""
Transformation Rule:

1. Identify all distinct connected objects composed of red pixels (color 2).
2. For each red object:
    a. Calculate its size 'L' (number of pixels).
    b. Determine a maximum Manhattan distance threshold 'k' using the formula: k = floor((L - 1) / 2).
    c. For every white pixel (color 0) in the grid, calculate its minimum Manhattan distance to any pixel within the current red object.
    d. If this minimum distance is greater than 0 and less than or equal to 'k', change the white pixel's color to azure (color 8).
3. Keep the original red pixels unchanged. Process each red object independently, updating the grid progressively.
"""

def manhattan_distance(p1, p2):
    """Calculates the Manhattan distance between two points (tuples or lists)."""
    return abs(p1[0] - p2[0]) + abs(p1[1] - p2[1])

def transform(input_grid: np.ndarray) -> np.ndarray:
    """
    Applies the transformation rule to the input grid.

    For each red object, calculates a size-dependent Manhattan distance threshold 'k'.
    Colors white pixels azure if their minimum distance to that red object is > 0 and <= k.

    Args:
        input_grid: A 2D numpy array representing the input grid.

    Returns:
        A 2D numpy array representing the transformed output grid.
    """
    # Initialize output_grid as a copy of the input
    output_grid = np.copy(input_grid)
    height, width = input_grid.shape
    
    # Define colors
    red_color = 2
    white_color = 0
    azure_color = 8

    # Find connected components (objects) of red pixels.
    # Use 8-connectivity (diagonals included) by default.
    # structure = np.array([[1,1,1], [1,1,1], [1,1,1]]) # Explicit 8-connectivity
    # labeled_array, num_features = label(input_grid == red_color, structure=structure)
    # Default connectivity for label is sufficient here (usually type 2 which is 8-connectivity for 2D)
    labeled_array, num_features = label(input_grid == red_color)

    # If no red objects are found, return the original grid copy
    if num_features == 0:
        return output_grid

    # Iterate through each identified red object (labels are 1-based)
    for i in range(1, num_features + 1):
        # Get the coordinates of all pixels belonging to the current object
        object_pixels = np.argwhere(labeled_array == i)
        
        # Skip if somehow an empty object was labeled (shouldn't happen)
        if len(object_pixels) == 0: 
            continue

        # Calculate the size 'L' of the object
        L = len(object_pixels)
        
        # Calculate the Manhattan distance threshold 'k'
        k = math.floor((L - 1) / 2)

        # If k is less than 1, no azure pixels need to be added for this object
        if k < 1:
            continue

        # Iterate through every pixel in the grid
        for r in range(height):
            for c in range(width):
                # Check if the pixel in the *current state* of the output grid is white
                if output_grid[r, c] == white_color:
                    current_pos = (r, c)
                    min_dist = float('inf')
                    
                    # Calculate the minimum Manhattan distance from the current white pixel
                    # to any pixel within the *current* red object
                    for obj_pixel_coords in object_pixels:
                        # np.argwhere returns [row, col], compatible with manhattan_distance helper
                        dist = manhattan_distance(current_pos, obj_pixel_coords)
                        min_dist = min(min_dist, dist)
                        # Optimization: if min_dist becomes 1, we know it's the minimum possible > 0
                        # but k could be large, so finding the exact minimum is needed.
                        
                    # If the minimum distance is within the calculated threshold (and > 0),
                    # change the pixel color to azure in the output grid.
                    # Note: Check is > 0 to avoid coloring the object pixels themselves if they were somehow white.
                    if 0 < min_dist <= k:
                        output_grid[r, c] = azure_color

    # Return the modified grid
    return output_grid
```
```python
import numpy as np
from scipy.ndimage import label
import math
from collections import deque

"""
Transformation Rule:

1. Create a copy of the input grid to serve as the output grid.
2. Identify all distinct connected objects composed of red pixels (color 2), considering 8-way connectivity (including diagonals).
3. For each identified red object:
    a. Calculate its size 'L' (the number of pixels in the object).
    b. Calculate a propagation distance threshold 'k' using the formula: k = floor((L - 1) / 2).
    c. If k is less than 1, this object generates no azure pixels, so skip to the next object.
    d. Perform a Breadth-First Search (BFS) starting simultaneously from all pixels of the current red object:
        i. Initialize a queue with all pixel coordinates of the current red object, each associated with a distance of 0.
        ii. Initialize a 'visited' grid or set to keep track of pixels that have been processed (initially marking all non-white pixels as visited). This prevents revisiting pixels colored red or azure (either initially or by a previous object's propagation).
        iii. While the queue is not empty:
            - Dequeue a pixel coordinate (r, c) and its distance 'd'.
            - If 'd' is already equal to the threshold 'k', do not explore its neighbors from this path, as they would exceed the distance limit.
            - Otherwise, examine the four cardinal neighbors (up, down, left, right) of (r, c).
            - For each neighbor (nr, nc):
                - Check if it's within the grid boundaries.
                - Check if it has not been 'visited' yet.
                - If both checks pass (meaning it's within bounds and currently white):
                    - Mark the neighbor (nr, nc) as 'visited'.
                    - Change the color of the pixel at (nr, nc) in the output grid to azure (8).
                    - Enqueue the neighbor (nr, nc) with distance 'd + 1'.
4. After processing all red objects, return the modified output grid.
"""

def transform(input_grid: np.ndarray) -> np.ndarray:
    """
    Applies a transformation where diamond shapes (azure, color 8) are grown
    around red objects (color 2). The size of the diamond depends on the
    size 'L' of the red object, extending 'k' steps (Manhattan distance),
    where k = floor((L - 1) / 2).

    Args:
        input_grid: A 2D numpy array representing the input grid.

    Returns:
        A 2D numpy array representing the transformed output grid.
    """
    # Define colors
    white_color = 0
    red_color = 2
    azure_color = 8

    # Initialize output_grid as a copy of the input
    output_grid = np.copy(input_grid)
    height, width = input_grid.shape

    # Find connected components (objects) of red pixels.
    # Use 8-connectivity (structure includes diagonals)
    structure = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])
    labeled_array, num_features = label(input_grid == red_color, structure=structure)

    # If no red objects are found, return the original grid copy
    if num_features == 0:
        return output_grid

    # Initialize a visited grid. Mark all non-white pixels as initially visited.
    # This prevents BFS from entering red pixels or pixels colored azure by a previous object.
    visited = (output_grid != white_color)

    # Iterate through each identified red object (labels are 1-based)
    for i in range(1, num_features + 1):
        # Get the coordinates of all pixels belonging to the current object
        object_pixels_indices = np.argwhere(labeled_array == i)
        # Convert indices to tuples for easier handling in queue/set
        object_pixels = [tuple(p) for p in object_pixels_indices]

        # Skip if somehow an empty object was labeled
        if not object_pixels:
            continue

        # Calculate the size 'L' of the object
        L = len(object_pixels)

        # Calculate the Manhattan distance threshold 'k'
        k = math.floor((L - 1) / 2)

        # If k is less than 1, this object doesn't generate azure pixels
        if k < 1:
            continue

        # --- Perform BFS for the current object ---
        queue = deque()

        # Initialize the queue with all pixels of the current red object at distance 0
        # Note: We don't need to re-mark red pixels as visited here, as they
        # were already marked in the initial `visited` grid setup.
        for r, c in object_pixels:
            queue.append(((r, c), 0)) # Store ((row, col), distance)

        # Process the queue
        while queue:
            (r, c), dist = queue.popleft()

            # If current distance reaches k, don't explore further from this pixel
            if dist >= k:
                continue

            # Explore 4 cardinal neighbors
            for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:
                nr, nc = r + dr, c + dc

                # Check boundaries
                if 0 <= nr < height and 0 <= nc < width:
                    # Check if the neighbor is white and hasn't been visited
                    if not visited[nr, nc]:
                        # Mark as visited (to avoid processing again)
                        visited[nr, nc] = True
                        # Color the pixel azure in the output grid
                        output_grid[nr, nc] = azure_color
                        # Add the neighbor to the queue for further exploration
                        queue.append(((nr, nc), dist + 1))

    # Return the modified grid
    return output_grid
```
