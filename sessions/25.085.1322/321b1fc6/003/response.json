{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```python\n\"\"\"\n1. Initialize an output grid of the same dimensions as the input grid, filled entirely with the background color (white, 0).\n2. Identify all distinct connected groups of azure (8) pixels in the input grid. These are the 'markers'. If no markers exist, return the initial white output grid.\n3. Identify all distinct connected groups of pixels that are *not* white (0) and *not* azure (8). These are the 'source components'.\n4. Determine the 'template' pattern based on the source components:\n    a. If there are no source components, the template is effectively empty, and the output grid remains white.\n    b. If there is exactly one source component, the template is its 'relative_pattern' (the component's shape within its own bounding box, with other pixels as background 0).\n    c. If there are multiple source components:\n        i. Check if all source components have the exact same 'relative_pattern'.\n        ii. If YES (all identical): Find the source component whose top-left corner has the smallest row index, breaking ties with the smallest column index. The template is the 'relative_pattern' of this specific component.\n        iii. If NO (components differ): Find the single smallest bounding box that encloses *all* pixels of *all* source components. The template is the rectangular region copied directly from the *input grid* corresponding to this combined bounding box (this includes any background pixels within the box).\n5. For each identified marker:\n    a. Find the coordinate (row, column) of its top-leftmost pixel.\n    b. Copy the determined template onto the output grid, placing the template's top-left corner at the marker's top-left coordinate. Pixels from the template overwrite the output grid. Ensure the template is clipped if it extends beyond the grid boundaries.\n6. Return the final modified output grid.\n\"\"\"\n\nimport numpy as np\nfrom scipy.ndimage import label, find_objects\n\ndef find_connected_components(grid, target_colors=None):\n    \"\"\"\n    Finds all connected components for specified colors or all non-background colors.\n\n    Args:\n        grid (np.array): The input grid.\n        target_colors (list or set, optional): A collection of integer colors\n            to find objects for. If None, finds objects for all colors except 0.\n\n    Returns:\n        list: A list of dictionaries, where each dictionary represents an object\n              and contains 'color', 'pixels' (list of (r, c) tuples),\n              'bbox' (slice tuple), 'top_left' (r, c),\n              'relative_pattern' (np.array of the object within its bbox).\n              Returns empty list if no objects found.\n    \"\"\"\n    objects = []\n    if target_colors is None:\n        colors_present = np.unique(grid)\n        target_colors = [c for c in colors_present if c != 0]\n    if not target_colors:\n        return objects\n\n    processed_pixels = np.zeros_like(grid, dtype=bool) # Keep track of visited pixels\n\n    for color in target_colors:\n        # Create mask for the current color, considering only unprocessed pixels\n        mask = (grid == color) & (~processed_pixels)\n        if not np.any(mask):\n            continue\n\n        labeled_array, num_features = label(mask)\n        found_locs = find_objects(labeled_array)\n\n        for i in range(num_features):\n            obj_index = i + 1\n            obj_slice = found_locs[i]\n            \n            # Find pixels belonging to this component *globally*\n            obj_coords_global = np.argwhere(labeled_array == obj_index)\n\n            # Check if any part of this object has already been processed (shouldn't happen with current logic but safe check)\n            if np.any(processed_pixels[obj_coords_global[:, 0], obj_coords_global[:, 1]]):\n                continue\n\n            # Mark pixels as processed\n            processed_pixels[obj_coords_global[:, 0], obj_coords_global[:, 1]] = True\n            \n            obj_pixels_list = [tuple(coord) for coord in obj_coords_global]\n\n            # Calculate precise bounding box and top-left\n            min_r = np.min(obj_coords_global[:, 0])\n            min_c = np.min(obj_coords_global[:, 1])\n            max_r = np.max(obj_coords_global[:, 0])\n            max_c = np.max(obj_coords_global[:, 1])\n            bbox = (slice(min_r, max_r + 1), slice(min_c, max_c + 1))\n            top_left = (min_r, min_c)\n            \n            # Create relative pattern (object shape in its bbox, rest 0)\n            height = max_r - min_r + 1\n            width = max_c - min_c + 1\n            relative_pattern = np.zeros((height, width), dtype=grid.dtype)\n            for r, c in obj_pixels_list:\n                relative_pattern[r - min_r, c - min_c] = color\n\n            objects.append({\n                'color': color,\n                'pixels': obj_pixels_list,\n                'bbox': bbox,\n                'top_left': top_left,\n                'relative_pattern': relative_pattern\n            })\n\n    # Sort objects primarily by top-left row, then column for consistent ordering\n    objects.sort(key=lambda o: (o['top_left'][0], o['top_left'][1]))\n    return objects\n\n\ndef get_combined_bounding_box(objects):\n    \"\"\" Calculates the minimal bounding box containing all pixels of given objects. \"\"\"\n    if not objects:\n        return None\n    min_r, min_c = float('inf'), float('inf')\n    max_r, max_c = float('-inf'), float('-inf')\n    pixel_found = False\n    for obj in objects:\n        for r, c in obj['pixels']:\n            pixel_found = True\n            min_r = min(min_r, r)\n            min_c = min(min_c, c)\n            max_r = max(max_r, r)\n            max_c = max(max_c, c)\n    if not pixel_found:\n        return None\n    return slice(min_r, max_r + 1), slice(min_c, max_c + 1)\n\ndef are_patterns_identical(pattern1, pattern2):\n    \"\"\" Checks if two numpy arrays representing patterns are identical. \"\"\"\n    if pattern1.shape != pattern2.shape:\n        return False\n    return np.array_equal(pattern1, pattern2)\n\ndef stamp_pattern(output_grid, pattern, top_left_coord):\n    \"\"\" Overlays the pattern onto the output grid at the specified coordinate. \"\"\"\n    r_start, c_start = top_left_coord\n    if r_start < 0 or c_start < 0: # Avoid stamping outside grid from negative indices\n        # Adjust pattern and start coords if starting negatively\n        # This scenario might indicate an issue elsewhere, but let's clip defensively\n         p_r_offset = 0\n         p_c_offset = 0\n         if r_start < 0:\n             p_r_offset = -r_start\n             r_start = 0\n         if c_start < 0:\n             p_c_offset = -c_start\n             c_start = 0\n         pattern = pattern[p_r_offset:, p_c_offset:] # Clip pattern from top/left\n\n\n    p_h, p_w = pattern.shape\n    g_h, g_w = output_grid.shape\n\n    # Calculate the intersection bounds\n    r_end = min(r_start + p_h, g_h)\n    c_end = min(c_start + p_w, g_w)\n\n    # Calculate slices for pattern and grid based on intersection\n    p_r_slice = slice(0, r_end - r_start)\n    p_c_slice = slice(0, c_end - c_start)\n    g_r_slice = slice(r_start, r_end)\n    g_c_slice = slice(c_start, c_end)\n\n    # Ensure slices are valid (have positive range) before attempting assignment\n    if g_r_slice.start < g_r_slice.stop and g_c_slice.start < g_c_slice.stop:\n        output_grid[g_r_slice, g_c_slice] = pattern[p_r_slice, p_c_slice]\n\n\ndef transform(input_grid):\n    \"\"\"\n    Transforms the input grid by stamping a derived pattern at locations marked by azure pixels.\n    \"\"\"\n    input_grid = np.array(input_grid, dtype=int)\n    output_grid = np.zeros_like(input_grid)\n    \n    # 1. Find marker objects (color 8)\n    marker_color = 8\n    marker_objects = find_connected_components(input_grid, target_colors=[marker_color])\n\n    # 2. Find source objects (all colors except 0 and 8)\n    all_colors = np.unique(input_grid)\n    source_colors = [c for c in all_colors if c != 0 and c != marker_color]\n    source_objects = find_connected_components(input_grid, target_colors=source_colors)\n\n    # 3. Handle edge cases: no markers or no source objects\n    if not marker_objects:\n        return output_grid.tolist() # Return empty grid if no markers\n    if not source_objects:\n        return output_grid.tolist() # Return empty grid if no source pattern\n\n    # 4. Determine the template pattern\n    template = None\n    if len(source_objects) == 1:\n        # Case: Single source object\n        template = source_objects[0]['relative_pattern']\n    else:\n        # Case: Multiple source objects\n        first_pattern = source_objects[0]['relative_pattern']\n        all_identical = True\n        for i in range(1, len(source_objects)):\n            if not are_patterns_identical(first_pattern, source_objects[i]['relative_pattern']):\n                all_identical = False\n                break\n\n        if all_identical:\n            # Subcase: All source objects are identical - use the top-leftmost one's pattern\n            # Objects are already sorted by top-left position in find_connected_components\n            template = source_objects[0]['relative_pattern']\n        else:\n            # Subcase: Source objects differ - use combined bounding box from input grid\n            combined_bbox = get_combined_bounding_box(source_objects)\n            if combined_bbox:\n                template = input_grid[combined_bbox]\n            else:\n                 # This should not happen if source_objects is not empty\n                 return output_grid.tolist() \n\n    if template is None: # Should not happen if source_objects exist\n         return output_grid.tolist()\n\n    # 5. Stamp the template for each marker\n    for marker in marker_objects:\n        marker_tl = marker['top_left']\n        stamp_pattern(output_grid, template, marker_tl)\n\n    # 6. Return the final grid\n    return output_grid.tolist()\n```"
          }
        ],
        "role": "model"
      },
      "finish_reason": 1,
      "index": 0,
      "safety_ratings": [],
      "token_count": 0,
      "grounding_attributions": [],
      "avg_logprobs": 0.0
    }
  ],
  "usage_metadata": {
    "prompt_token_count": 7847,
    "candidates_token_count": 2649,
    "total_token_count": 10496,
    "cached_content_token_count": 0
  },
  "model_version": "gemini-2.5-pro-exp-03-25",
  "response_time": 27.843575,
  "retries": null
}