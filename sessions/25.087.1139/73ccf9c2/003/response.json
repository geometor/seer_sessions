{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```python\nimport numpy as np\nfrom collections import deque\n\n\"\"\"\nThe transformation identifies all connected objects of the single non-white color in the input grid.\nIt categorizes these objects into 'signal' (area > 1) and 'noise' (area = 1).\nIf the count of noise objects is strictly greater than the count of signal objects, \nthe target object is the noise object (single pixel) that is located bottom-most, \nand then right-most in case of a tie in the row coordinate.\nOtherwise (if the count of noise objects is less than or equal to the count of signal objects), \nthe target object is the signal object whose bounding box's top-left corner is \nlocated bottom-most (maximum row index), and then right-most (maximum column index) \nin case of a tie.\nThe final output grid contains only the target object, cropped to its minimal bounding box,\nwith the remaining space filled with the background color (white, 0).\n\"\"\"\n\ndef find_objects(grid, target_color):\n    \"\"\"\n    Finds all connected objects of a specific color in the grid using BFS.\n\n    Args:\n        grid (np.array): The input grid.\n        target_color (int): The color of the objects to find.\n\n    Returns:\n        list: A list of objects, where each object is a set of (row, col) tuples.\n    \"\"\"\n    rows, cols = grid.shape\n    visited = set()\n    objects = []\n\n    for r in range(rows):\n        for c in range(cols):\n            if grid[r, c] == target_color and (r, c) not in visited:\n                # Start BFS to find a connected object\n                obj_pixels = set()\n                q = deque([(r, c)])\n                visited.add((r, c))\n                obj_pixels.add((r, c))\n\n                while q:\n                    row, col = q.popleft()\n\n                    # Check neighbors (4-connectivity: up, down, left, right)\n                    for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n                        nr, nc = row + dr, col + dc\n                        if 0 <= nr < rows and 0 <= nc < cols and \\\n                           grid[nr, nc] == target_color and (nr, nc) not in visited:\n                            visited.add((nr, nc))\n                            obj_pixels.add((nr, nc))\n                            q.append((nr, nc))\n                \n                if obj_pixels:\n                    objects.append(obj_pixels)\n    return objects\n\ndef get_bounding_box(obj_pixels):\n    \"\"\"\n    Calculates the bounding box for a set of pixels.\n\n    Args:\n        obj_pixels (set): A set of (row, col) tuples representing an object.\n\n    Returns:\n        tuple: (min_row, min_col, max_row, max_col) or None if the set is empty.\n    \"\"\"\n    if not obj_pixels:\n        return None\n    \n    rows = [r for r, c in obj_pixels]\n    cols = [c for r, c in obj_pixels]\n    \n    min_row = min(rows)\n    min_col = min(cols)\n    max_row = max(rows)\n    max_col = max(cols)\n    \n    return min_row, min_col, max_row, max_col\n\ndef transform(input_grid):\n    \"\"\"\n    Identifies a target object based on signal/noise counts and position,\n    then extracts it into its bounding box.\n    \"\"\"\n    grid = np.array(input_grid, dtype=int)\n    rows, cols = grid.shape\n    \n    # Find the non-white color present in the grid\n    unique_colors = np.unique(grid)\n    target_color = -1\n    for color in unique_colors:\n        if color != 0:\n            target_color = color\n            break\n            \n    # Handle cases with no non-white color or empty grid\n    if target_color == -1:\n        return [[0]] \n\n    # Find all objects of the target color\n    all_objects = find_objects(grid, target_color)\n    \n    if not all_objects:\n        # No objects of the target color found\n         return [[0]]\n\n    # Categorize objects into signal (area > 1) and noise (area == 1)\n    signal_objects = []\n    noise_objects_coords = [] # Store (r, c) for noise objects for sorting\n    noise_objects_pixels = [] # Store pixel set for noise objects\n\n    for obj_pixels in all_objects:\n        if len(obj_pixels) == 1:\n            # It's a noise object, store its coordinate and pixel set\n            coord = next(iter(obj_pixels)) # Get the single (r, c) tuple\n            noise_objects_coords.append(coord)\n            noise_objects_pixels.append(obj_pixels)\n        else:\n            # It's a signal object\n            signal_objects.append(obj_pixels)\n\n    N_signal = len(signal_objects)\n    N_noise = len(noise_objects_coords)\n    \n    selected_object_pixels = None\n\n    # Selection Logic\n    if N_noise > N_signal:\n        # Case 1: More noise than signal\n        # Sort noise objects: bottom-most (max r), then right-most (max c)\n        # Combine coordinates and pixels for sorting\n        noise_data = sorted(zip(noise_objects_coords, noise_objects_pixels), \n                            key=lambda item: (item[0][0], item[0][1]), \n                            reverse=True) \n        if noise_data:\n            selected_object_pixels = noise_data[0][1] # Get the pixel set of the selected noise object\n            \n    else:\n        # Case 2: Noise <= Signal (or no signal objects, which shouldn't happen if N_noise <= N_signal and all_objects is not empty)\n        if not signal_objects: \n             # This case might occur if N_noise=0 and N_signal=0, already handled by 'if not all_objects'\n             # Or if N_noise > 0 and N_signal=0, handled by the N_noise > N_signal case.\n             # If somehow N_noise <= N_signal but N_signal is 0, default to selecting based on noise if any.\n             if noise_objects_coords:\n                 noise_data = sorted(zip(noise_objects_coords, noise_objects_pixels), \n                                     key=lambda item: (item[0][0], item[0][1]), \n                                     reverse=True)\n                 selected_object_pixels = noise_data[0][1]\n             else: # Should not be reachable if all_objects was not empty\n                 return [[0]] \n        else:\n            # Calculate bounding box top-left corners for signal objects\n            signal_data = []\n            for pixels in signal_objects:\n                bbox = get_bounding_box(pixels)\n                if bbox:\n                    min_r, min_c, _, _ = bbox\n                    signal_data.append({'pixels': pixels, 'min_r': min_r, 'min_c': min_c})\n            \n            # Sort signal objects: bottom-most (max min_r), then right-most (max min_c)\n            sorted_signal_data = sorted(signal_data, \n                                        key=lambda item: (item['min_r'], item['min_c']), \n                                        reverse=True)\n            \n            if sorted_signal_data:\n                 selected_object_pixels = sorted_signal_data[0]['pixels']\n\n    # If no object could be selected (edge case, e.g., only signal objects but bbox calculation failed)\n    if selected_object_pixels is None:\n        return [[0]] # Or handle appropriately\n\n    # Get the bounding box of the selected object\n    bbox = get_bounding_box(selected_object_pixels)\n    if bbox is None:\n         return [[0]] # Should not happen if selected_object_pixels is not None/empty\n         \n    min_r, min_c, max_r, max_c = bbox\n    \n    # Calculate output grid dimensions\n    output_height = max_r - min_r + 1\n    output_width = max_c - min_c + 1\n    \n    # Initialize output grid with white (0)\n    output_grid = np.zeros((output_height, output_width), dtype=int)\n    \n    # Copy the selected object pixels to the output grid, adjusting coordinates\n    for r, c in selected_object_pixels:\n        output_r = r - min_r\n        output_c = c - min_c\n        # Ensure indices are within bounds (should be by definition of bbox)\n        if 0 <= output_r < output_height and 0 <= output_c < output_width:\n            output_grid[output_r, output_c] = target_color\n        \n    # Convert back to list of lists for the required output format\n    return output_grid.tolist()\n```"
          }
        ],
        "role": "model"
      },
      "finish_reason": 1,
      "index": 0,
      "safety_ratings": [],
      "token_count": 0,
      "grounding_attributions": [],
      "avg_logprobs": 0.0
    }
  ],
  "usage_metadata": {
    "prompt_token_count": 13638,
    "candidates_token_count": 2133,
    "total_token_count": 15771,
    "cached_content_token_count": 0
  },
  "model_version": "gemini-2.5-pro-exp-03-25",
  "response_time": 126.4654,
  "retries": null
}