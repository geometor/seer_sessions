
## train_1

**input:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 2 2 2 2 2 2 2 0 0 0 0
0 0 0 0 0 0 0 2 2 2 2 2 2 2 0 0 0 0
0 4 4 4 0 0 0 2 2 2 2 2 2 2 0 0 0 0
0 4 4 4 4 4 4 2 2 2 2 2 2 2 0 0 0 0
0 4 4 4 0 0 0 2 2 2 2 2 2 2 0 0 0 0
0 0 0 0 0 0 0 2 2 2 2 2 2 2 0 0 0 0
0 0 0 0 0 0 0 2 2 2 2 2 2 2 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


**output:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 2 2 2 2 2 2 2 0 0 0 0
0 0 0 0 0 0 0 2 2 2 2 2 2 2 0 0 0 0
0 0 0 0 4 4 4 2 2 2 2 2 2 2 0 0 0 0
0 0 0 0 4 4 4 2 2 2 2 2 2 2 0 0 0 0
0 0 0 0 4 4 4 2 2 2 2 2 2 2 0 0 0 0
0 0 0 0 0 0 0 2 2 2 2 2 2 2 0 0 0 0
0 0 0 0 0 0 0 2 2 2 2 2 2 2 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


## train_2

**input:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 4 4 4 4 4 4 4 0 0 0 0 0 0
0 4 4 4 4 4 4 4 0 0 0 0 0 0
0 4 4 4 4 4 4 4 0 0 0 0 0 0
0 0 0 5 0 0 0 0 0 0 0 0 0 0
0 0 0 5 0 0 0 0 0 0 0 0 0 0
0 0 0 5 0 0 0 0 0 0 0 0 0 0
0 0 0 5 0 0 0 0 0 0 0 0 0 0
0 0 5 5 5 5 0 0 0 0 0 0 0 0
0 0 5 5 5 5 0 0 0 0 0 0 0 0
0 0 5 5 5 5 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


**output:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 4 4 4 4 4 4 4 0 0 0 0 0 0
0 4 4 4 4 4 4 4 0 0 0 0 0 0
0 4 4 4 4 4 4 4 0 0 0 0 0 0
0 0 5 5 5 5 0 0 0 0 0 0 0 0
0 0 5 5 5 5 0 0 0 0 0 0 0 0
0 0 5 5 5 5 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


## train_3

**input:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 2 2 2 2 2 0 0 0 0 0 0 0 0
0 0 0 0 0 0 2 2 2 2 2 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0
0 0 0 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0
0 0 0 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0
0 0 0 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0
0 0 0 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0
0 0 0 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0
0 0 0 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```


**output:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 2 2 2 2 2 0 0 0 0 0 0 0 0
0 0 0 0 0 0 2 2 2 2 2 0 0 0 0 0 0 0 0
0 0 0 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0
0 0 0 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0
0 0 0 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0
0 0 0 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0
0 0 0 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0
0 0 0 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```

Okay, analyzing the provided examples.

**Perception:**

1.  **Input Grid Structure:** Each input grid contains a white background (color 0) and two distinct objects of different non-white colors.
2.  **Object Colors:** One object is consistently red (color 2). The other object's color varies across examples (yellow=4, gray=5, green=3).
3.  **Object Shapes:** The shapes are varied. The red object is always a solid rectangle. The other object has different forms ('plus' shape, 'L' shape, 'H' shape).
4.  **Spatial Relationship:** In all examples, the non-red object partially overlaps or intersects with the red object.
5.  **Output Grid Structure:** The output grid retains the red object exactly as it was in the input. The non-red object is modified.
6.  **Transformation:** The non-red object appears to be "clipped" or "masked" by the red object. Only the pixels of the non-red object that occupy the same position as pixels of the red object in the input grid are preserved in the output grid. All other pixels of the original non-red object are turned white (0). The red object itself remains unchanged.

**YAML Facts:**


```yaml
task_description: "Clip one object based on the spatial extent of a specific 'mask' object (red)."

examples:
  train_1:
    input:
      objects:
        - id: obj1
          color: yellow
          shape: plus_sign
          pixels: [(5,1), (5,2), (5,3), (6,1), (6,2), (6,3), (6,4), (6,5), (6,6), (7,1), (7,2), (7,3)]
        - id: obj2
          color: red
          shape: rectangle
          pixels: [(3,7)-(9,13)] # Using range notation for brevity
      relationships:
        - type: overlap
          objects: [obj1, obj2]
          overlapping_pixels_obj1: [(5,7)-(7,7), (6,8)-(6,13), (7,8)-(7,13)] # These are relative to obj1's local coords if considering clipping
          overlapping_pixels_grid: [(5,7), (6,7), (7,7), (6,8), (6,9), (6,10), (6,11), (6,12), (6,13), (7,8), (7,9), (7,10), (7,11), (7,12), (7,13)] # Corrected to grid coords
    output:
      objects:
        - id: obj1_clipped
          color: yellow
          pixels: [(5,7), (6,7), (7,7), (6,8), (6,9), (6,10), (6,11), (6,12), (6,13), (7,8), (7,9), (7,10), (7,11), (7,12), (7,13)] # Pixels from obj1 that coincided with obj2
        - id: obj2_preserved
          color: red
          pixels: [(3,7)-(9,13)] # Same as input
      action:
        - type: preserve
          target: obj2 (red object)
        - type: clip
          target: obj1 (yellow object)
          mask: obj2 (red object)
          result: obj1_clipped

  train_2:
    input:
      objects:
        - id: obj1
          color: yellow
          shape: rectangle
          pixels: [(1,1)-(3,7)]
        - id: obj2
          color: gray
          shape: L_shape
          pixels: [(4,3), (5,3), (6,3), (7,3), (8,2)-(8,5), (9,2)-(9,5), (10,2)-(10,5)]
      relationships:
        - type: overlap
          objects: [obj1, obj2]
          overlapping_pixels_grid: [(4,3), (5,3), (6,3), (7,3)] # Pixels of obj2 overlapping with obj1's area
    output:
      objects:
        - id: obj1_preserved
          color: yellow
          pixels: [(1,1)-(3,7)]
        - id: obj2_clipped
          color: gray
          pixels: [(4,3), (5,3), (6,3), (7,3)] # Only overlapping pixels remain
      action:
        - type: preserve
          target: obj1 (yellow object) # Note: The role seems color-dependent, red is the mask. Let's relabel based on role.
        - type: clip
          target: obj2 (gray object)
          mask: obj1 (yellow object) # Re-evaluating: It seems the RED object is consistently the mask. Train 2 might be an anomaly or I misinterpreted the rule. Re-checking train_1 and train_3. Yes, RED is the mask. Train 2 seems different.

  # Re-evaluating train_2 based on train_1 and train_3's pattern:
  # If RED is always the mask, then train_2 might have a different rule or the mask color isn't always red.
  # Let's assume the pattern holds: Red is the mask, the other color is clipped.
  # Re-examining train_2: Gray (5) is clipped by Yellow (4). This contradicts the Red (2) mask hypothesis.
  # Alternative Hypothesis: The object that is *lower* down or *more central* acts as the mask? Or maybe the object with the simplest shape (rectangle)?
  # Let's re-examine all three:
  # Train 1: Red (2) rectangle masks Yellow (4) plus. Red is preserved.
  # Train 2: Yellow (4) rectangle masks Gray (5) L-shape. Yellow is preserved.
  # Train 3: Red (2) rectangle masks Green (3) H-shape. Red is preserved.

  # Revised Hypothesis: The RECTANGULAR object acts as the mask, regardless of its color. The other, more complex shape is clipped to the bounds of the rectangle. The rectangle itself is always preserved.

  # Revised YAML for train_2 based on the RECTANGLE MASK hypothesis:
  train_2_revised:
    input:
      objects:
        - id: obj_mask # rectangle
          color: yellow
          shape: rectangle
          pixels: [(1,1)-(3,7)]
        - id: obj_clip # L-shape
          color: gray
          shape: L_shape
          pixels: [(4,3), (5,3), (6,3), (7,3), (8,2)-(8,5), (9,2)-(9,5), (10,2)-(10,5)]
      relationships:
        - type: overlap
          objects: [obj_mask, obj_clip]
          overlapping_pixels_grid: [(4,3), (5,3), (6,3), (7,3)] # Pixels of obj_clip overlapping with obj_mask's area
    output:
      objects:
        - id: obj_mask_preserved
          color: yellow
          pixels: [(1,1)-(3,7)] # Preserved
        - id: obj_clip_clipped
          color: gray
          pixels: [(4,3), (5,3), (6,3), (7,3)] # Clipped pixels
      action:
        - type: preserve
          target: obj_mask (yellow rectangle)
        - type: clip
          target: obj_clip (gray L-shape)
          mask: obj_mask (yellow rectangle)
          result: obj_clip_clipped

  train_3:
    input:
      objects:
        - id: obj_mask # rectangle
          color: red
          shape: rectangle
          pixels: [(1,6)-(2,10)]
        - id: obj_clip # H-shape
          color: green
          shape: H_shape
          pixels: [(3,7), (4,7), (5,7), (6,7), (7,3)-(7,10), (8,3)-(8,10), (9,3)-(9,10), (10,3)-(10,10), (11,3)-(11,10), (12,3)-(12,10)]
      relationships:
        - type: overlap
          objects: [obj_mask, obj_clip]
          overlapping_pixels_grid: [(3,7), (4,7), (5,7), (6,7), (7,6)-(7,10), (8,6)-(8,10), (9,6)-(9,10), (10,6)-(10,10), (11,6)-(11,10), (12,6)-(12,10)] # Pixels of obj_clip overlapping with obj_mask's area
    output:
      objects:
        - id: obj_mask_preserved
          color: red
          pixels: [(1,6)-(2,10)] # Preserved
        - id: obj_clip_clipped
          color: green
          pixels: [(3,7), (4,7), (5,7), (6,7), (7,6)-(7,10), (8,6)-(8,10), (9,6)-(9,10), (10,6)-(10,10), (11,6)-(11,10), (12,6)-(12,10)] # Clipped pixels
      action:
        - type: preserve
          target: obj_mask (red rectangle)
        - type: clip
          target: obj_clip (green H-shape)
          mask: obj_mask (red rectangle)
          result: obj_clip_clipped

general_rule:
  - Identify all distinct non-white objects in the input grid.
  - Determine which object is a solid rectangle (the mask object).
  - Determine which object is not a solid rectangle (the object to be clipped).
  - Create the output grid, initially identical to the input grid.
  - Preserve all pixels belonging to the mask object (the rectangle).
  - For the object to be clipped: iterate through its pixels. If a pixel's location (row, column) corresponds to a location *outside* the mask object in the input grid, change that pixel's color to white (0) in the output grid. If the pixel's location is *inside* or *on the boundary* of the mask object, keep its original color.

```


**Natural Language Program:**

1.  Initialize the output grid as a copy of the input grid.
2.  Identify all contiguous regions of non-white pixels in the input grid. These are the objects.
3.  Identify the object that forms a solid rectangle. This is the 'mask' object. There will be exactly one such object.
4.  Identify the other non-white object. This is the 'target' object.
5.  Iterate through each pixel (cell) of the input grid belonging to the 'target' object.
6.  For each pixel of the 'target' object, check if the corresponding pixel location in the input grid belongs to the 'mask' object (i.e., check if `input_grid[row][col]` has the color of the mask object).
7.  If the target object's pixel location *does not* correspond to a pixel of the mask object in the input grid, change the color of that pixel in the *output* grid to white (0).
8.  Leave all pixels belonging to the mask object and the background unchanged in the output grid (relative to the initial copy). The pixels of the target object that *do* overlap with the mask object also remain unchanged from the initial copy.
