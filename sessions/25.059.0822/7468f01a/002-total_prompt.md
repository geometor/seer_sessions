# 7468f01a • 002 • example_3 • investigate_coder

---

## train_1

**input:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 8 8 8 8 8 8 2 8 0 0 0 0 0
0 0 8 2 2 8 8 8 8 8 0 0 0 0 0
0 0 8 8 2 2 8 8 8 8 0 0 0 0 0
0 0 8 8 8 8 8 8 8 8 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```

<PIL.Image.Image image mode=RGB size=960x640 at 0x70A2ECD5CBD0>

**output:**
```
8 2 8 8 8 8 8 8
8 8 8 8 8 2 2 8
8 8 8 8 2 2 8 8
8 8 8 8 8 8 8 8
```

<PIL.Image.Image image mode=RGB size=512x256 at 0x70A2ECD5DE50>

## train_2

**input:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 4 4 4 4 4 0 0 0 0 0 0 0 0
0 0 0 4 4 4 4 4 0 0 0 0 0 0 0 0
0 0 0 4 1 1 4 4 0 0 0 0 0 0 0 0
0 0 0 4 4 1 1 4 0 0 0 0 0 0 0 0
0 0 0 4 4 1 4 4 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```

<PIL.Image.Image image mode=RGB size=1024x768 at 0x70A2ECD5DD50>

**output:**
```
4 4 4 4 4
4 4 4 4 4
4 4 1 1 4
4 1 1 4 4
4 4 1 4 4
```

<PIL.Image.Image image mode=RGB size=320x320 at 0x70A2EC683A50>

## train_3

**input:**
```
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 6 6 6 3 6 6 0 0 0 0 0 0 0 0 0
0 0 6 3 3 3 6 6 0 0 0 0 0 0 0 0 0
0 0 6 6 6 6 3 6 0 0 0 0 0 0 0 0 0
0 0 6 6 6 6 3 6 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```

<PIL.Image.Image image mode=RGB size=1088x960 at 0x70A2ECDDCAD0>

**output:**
```
6 6 3 6 6 6
6 6 3 3 3 6
6 3 6 6 6 6
6 3 6 6 6 6
```

<PIL.Image.Image image mode=RGB size=384x256 at 0x70A2EC681DD0>
<PIL.Image.Image image mode=RGB size=3200x1360 at 0x70A2EC682FD0>Okay, here's a breakdown of my observations, documented facts, and a natural language program describing the transformation:

**Perception of Elements:**

The task involves extracting a contiguous, non-white colored region from a larger grid.  The extracted region appears to maintain the relative positions and colors of its pixels.  The output grid is a minimal bounding box around the extracted region. Background (white/0) pixels are discarded. The input grids contain a single connected component of non-white pixels. Within the colored region, there appears to be a secondary color, different from the primary color making up the outer shape.

**YAML Fact Block:**


```yaml
facts:
  - observation: |
      Input grids contain a single connected region of non-white pixels.
  - observation: |
      The non-white region contains pixels of at least two different colors.
  - observation: |
      One color forms the outer shape of the connected object.
  - observation: |
      Another different color is contained within the shape.
  - observation: |
      The output is a rectangular subgrid.
  - observation: |
      The output subgrid contains only the non-white pixels from the input.
  - observation: |
      The spatial arrangement of the non-white pixels is preserved in the output.
  - observation: |
      The output grid is the smallest possible rectangle that contains all the non-white pixels.
  - relationship: |
      Output grid dimensions are determined by the extreme row and column indices of the non-white pixels in the input grid.
```


**Natural Language Program:**

1.  **Identify the bounding box:** Find the minimum and maximum row and column indices of all non-white (non-zero) pixels within the input grid.
2.  **Extract the subgrid:** Create a new, smaller grid based on the bounding box dimensions found in step 1.
3.  **Copy pixels:** Iterate over the non-white pixels within the input grid. Copy the pixel values to their corresponding positions in the new, smaller output grid, using relative row and column positions within the bounding box.
4. return the subgrid

In simpler term, the algorithm crops out the non-white part of the input image.

**INSTRUCTIONS**

- generate a Python module to represent the natural language program 
- follow the template below
- all common math and science libraries are available
- create docstring only at top - but comment the workflow in the function
- use helper functions for common operations like finding objects, counting,
  measuring, moving, etc. 

remember - we are working iteratively - so this code will inform your future
self as we converge on the solution

*template:*

```python
"""
{{ natural language description of the transformation rule }}
"""

{{ imports }}

def transform(input_grid):
    # initialize output_grid

    # change output pixels 

    return output_grid

```
