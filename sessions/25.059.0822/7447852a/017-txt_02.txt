import numpy as np

def compare_grids(grid1, grid2):
    """
    Compares two grids and returns a list of coordinates where they differ,
    along with the values at those coordinates.
    """
    if grid1.shape != grid2.shape:
        return "Grids have different shapes"

    differences = []
    for i in range(grid1.shape[0]):
        for j in range(grid1.shape[1]):
            if grid1[i, j] != grid2[i, j]:
                differences.append(((i, j), grid1[i, j], grid2[i, j]))
    return differences

def show_grid(grid_name, grid):
    if isinstance(grid, str):
        print(f"{grid_name}: {grid}")
    else:
        print(f"{grid_name}:")
        print(grid)

def run_comparison(task):
    all_metrics = {}
    for example_index in range(len(task['train'])):
      input_grid = np.array(task['train'][example_index]['input'])
      expected_output = np.array(task['train'][example_index]['output'])
      transformed_output = transform(input_grid)
      
      show_grid(f"Example {example_index + 1} - Input", input_grid)
      show_grid(f"Example {example_index + 1} - Expected Output", expected_output)
      show_grid(f"Example {example_index + 1} - Transformed Output", transformed_output)

      differences = compare_grids(transformed_output, expected_output)

      metrics = {
          'input_shape': input_grid.shape,
          'output_shape': expected_output.shape,
          'transformed_output_shape': transformed_output.shape,
          'differences': differences
      }
      all_metrics[f'example_{example_index + 1}'] = metrics
    return all_metrics

# Paste the task data here - replace with actual data when available
task_data = {
  "train": [
    {
      "input": [
        [0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 2, 0, 2, 0, 2, 0, 2, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0]
      ],
      "output": [
        [0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 2, 4, 2, 4, 2, 4, 2, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0]
      ]
    },
    {
      "input": [
        [0, 0, 0, 0, 0],
        [0, 2, 0, 2, 0],
        [0, 0, 0, 0, 0],
        [0, 2, 0, 2, 0],
        [0, 0, 0, 0, 0]
      ],
      "output": [
        [0, 0, 0, 0, 0],
        [0, 2, 4, 2, 0],
        [0, 0, 0, 0, 0],
        [0, 2, 4, 2, 0],
        [0, 0, 0, 0, 0]
      ]
    },
    {
      "input": [
        [0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 2, 0, 2, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 2, 0, 2, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0]
      ],
      "output": [
        [0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 2, 4, 2, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 2, 4, 2, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0]
      ]
    },
        {
      "input": [
        [2, 0, 2],
        [0, 0, 0],
        [2, 0, 2]
      ],
      "output": [
        [2, 4, 2],
        [4, 4, 4],
        [2, 4, 2]
      ]
    }
  ]
}

results = run_comparison(task_data)
for example, metrics in results.items():
    print(f"\\n{example}:")
    for key, value in metrics.items():
        print(f"  {key}: {value}")