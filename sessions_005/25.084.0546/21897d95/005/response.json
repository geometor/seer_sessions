{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "```python\n\"\"\"\n1.  **Identify Framed Object:**\n    *   Find the object in the *input* grid whose shape is most closely replicated (with possible color change, and filling) within the *output* grid. Consider multiple disconnected input components of the same color as potentially forming *one* object. This is our \"framed object\".\n2.  **Identify Frame:**\n    * Examine the *output* grid.\n    * Find the largest rectangular contiguous region of pixels that \"encloses\" or \"frames\" a differently colored region (the transformed \"framed object\") within the output grid.\n    * Find the colors in the *input* grid that *do not* take part of the \"framed object\".\n3.  **Output Grid Construction:**\n    *   The output grid's dimensions are defined by either the \"frame\" shape or the \"framed object,\" whichever results in the largest complete rectangle that encompasses both.\n    *   Populate the \"frame\" in the output using one of the colors that was *not* part of the \"framed object\" in the input.  The shape is maintained (or slightly modified to become a complete frame).\n    *   Populate the \"framed object\" area in the output. This involves:\n        *   Identifying the new color of the framed object (a color present in the output but not the input, if a new color is introduced).\n        *   Filling the bounding box of the framed object's original shape with this new color (or a combination of new colors from the output).\n\"\"\"\n\nimport numpy as np\nfrom collections import defaultdict\n\ndef find_objects(grid):\n    \"\"\"\n    Finds connected components (objects) in a grid, including single pixels.\n    Returns a dictionary where keys are colors and values are lists of (row, col) tuples.\n    \"\"\"\n    objects = defaultdict(list)\n    visited = set()\n    rows, cols = grid.shape\n\n    def dfs(r, c, color):\n        if (r, c) in visited or r < 0 or r >= rows or c < 0 or c >= cols or grid[r, c] != color:\n            return []\n        visited.add((r, c))\n        obj = [(r, c)]\n        obj.extend(dfs(r + 1, c, color))\n        obj.extend(dfs(r - 1, c, color))\n        obj.extend(dfs(r, c + 1, color))\n        obj.extend(dfs(r, c - 1, color))\n        return obj\n\n    for r in range(rows):\n        for c in range(cols):\n            if (r, c) not in visited:\n                color = grid[r, c]\n                objects[color].extend(dfs(r,c,color)) # append pixels to existing\n    return dict(objects)\n\n\ndef bounding_box(pixels):\n    \"\"\"\n    Calculates the bounding box of a set of pixels.\n    Returns ((min_row, min_col), (max_row, max_col)).\n    \"\"\"\n    if not pixels:\n        return ((0, 0), (0, 0))\n    rows, cols = zip(*pixels)\n    return ((min(rows), min(cols)), (max(rows), max(cols)))\n\ndef shape_similarity(pixels1, pixels2):\n    \"\"\"\n    A very basic shape similarity metric.  Counts the number of pixels\n    that overlap when the centroids of the two pixel sets are aligned.\n    \"\"\"\n    if not pixels1 or not pixels2:\n        return 0\n\n    # Calculate centroids\n    centroid1_row = sum(r for r, _ in pixels1) / len(pixels1)\n    centroid1_col = sum(c for _, c in pixels1) / len(pixels1)\n    centroid2_row = sum(r for r, _ in pixels2) / len(pixels2)\n    centroid2_col = sum(c for _, c in pixels2) / len(pixels2)\n\n    # Translate pixels to align centroids\n    translated_pixels1 = {(r - centroid1_row, c - centroid1_col) for r, c in pixels1}\n    translated_pixels2 = {(r - centroid2_row, c - centroid2_col) for r, c in pixels2}\n\n    # Calculate overlap\n    overlap = len(translated_pixels1.intersection(translated_pixels2))\n    return overlap\n\n\ndef get_frame_and_framed_colors(input_objects, expected_output_objects):\n\n    best_framed_color = None\n    best_frame_color = None\n    max_similarity = -1\n    output_colors = set(expected_output_objects.keys())\n\n    # find colors and combination of colors that best match output\n    for input_color, input_pixels in input_objects.items():\n            for output_color, output_pixels in expected_output_objects.items():\n              similarity = shape_similarity(input_pixels, output_pixels)\n              if similarity > max_similarity:\n                    max_similarity = similarity\n                    best_framed_color = input_color\n                    \n    # find colors that do not take part in the framed object\n    frame_colors = [c for c in input_objects if c != best_framed_color]\n    if len(frame_colors) > 0:\n      best_frame_color = frame_colors[0]\n\n    return best_framed_color, best_frame_color, output_colors\ndef transform(input_grid):\n    input_objects = find_objects(input_grid)\n\n    # Dummy expected output objects for development purposes.  In a real\n    # scenario, we wouldn't have access to the expected output during\n    # the transformation. We use it here *only* to infer the rule.\n    # Create dummy output grid to simulate\n    expected_output_grid = np.zeros_like(input_grid)\n    expected_output_objects = find_objects(expected_output_grid)\n\n    # get dimensions\n    rows, cols = input_grid.shape\n    output_grid = np.zeros((rows, cols), dtype=int)\n\n    # Identify framed and frame object\n    framed_color, frame_color, output_colors = get_frame_and_framed_colors(input_objects, expected_output_objects)\n\n    if framed_color is None or frame_color is None:\n        return output_grid\n\n    # 3. **Output Grid Construction:**\n    # Get pixels\n    framed_pixels = input_objects.get(framed_color,[])\n    frame_pixels = input_objects.get(frame_color,[])\n\n    # determine output size\n    framed_bbox = bounding_box(framed_pixels)\n    frame_bbox = bounding_box(frame_pixels)\n    out_max_row = max(framed_bbox[1][0], frame_bbox[1][0])\n    out_max_col = max(framed_bbox[1][1], frame_bbox[1][1])\n\n    output_grid = np.zeros((out_max_row + 1, out_max_col + 1), dtype=int)\n\n    # fill frame\n    new_frame_color = frame_color # Default to same color\n    for color in output_colors:\n        if shape_similarity(frame_pixels,expected_output_objects[color]) > 0:\n            new_frame_color = color\n            break\n    for r, c in frame_pixels:\n        if r <= out_max_row and c <= out_max_col:  # Check bounds\n           output_grid[r, c] = new_frame_color\n\n    # fill framed\n    new_color = None\n    for color in output_colors:\n      if color != new_frame_color:\n          new_color = color\n\n    if new_color is not None:\n      framed_min_row, framed_min_col = framed_bbox[0]\n      framed_max_row, framed_max_col = framed_bbox[1]\n      for r in range(framed_min_row, framed_max_row + 1):\n          for c in range(framed_min_col, framed_max_col + 1):\n              if r <= out_max_row and c <= out_max_col:\n                output_grid[r, c] = new_color\n    return output_grid\n```"
          }
        ],
        "role": "model"
      },
      "finish_reason": 1,
      "index": 0,
      "safety_ratings": [],
      "token_count": 0,
      "grounding_attributions": [],
      "avg_logprobs": 0.0
    }
  ],
  "usage_metadata": {
    "prompt_token_count": 51391,
    "candidates_token_count": 1894,
    "total_token_count": 53285,
    "cached_content_token_count": 0
  },
  "model_version": "gemini-2.0-pro-exp-02-05",
  "response_time": 59.273769
}